{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68a90",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `introdl` module is already installed.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to ensure course package is installed\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "course_tools_path = Path('../../Lessons/Course_Tools/').resolve() # change this to the local path of the course package\n",
    "sys.path.append(str(course_tools_path))\n",
    "\n",
    "from install_introdl import ensure_introdl_installed\n",
    "ensure_introdl_installed(force_update=False, local_path_pkg= course_tools_path / 'introdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d43fa",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# imports and configuration\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b439c",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Homework 3 - Better Training\n",
    "\n",
    "In this assignment you will build a deeper CNN model to improve the classification performance on the FashionMNIST dataset.  Deeper models can be more difficult to train so you'll employ some of the techniques from Lesson 3 to improve the training.  You'll also use data augmentation to improve the performance of the model while reducing overfitting.  Along the way you'll see how to downsample a dataset to make for more efficient experimentaton.\n",
    "\n",
    "## Build the model (5 pts)\n",
    "\n",
    "Implement a PyTorch model of class `nn.module` to reproduce a model with this structure\n",
    "```\n",
    "====================================================================================================\n",
    "Layer (type (var_name))                  Input Shape          Output Shape         Param #\n",
    "====================================================================================================\n",
    "FashionMNISTModel (FashionMNISTModel)    [64, 1, 28, 28]      [64, 10]             --\n",
    "├─Sequential (block1)                    [64, 1, 28, 28]      [64, 32, 14, 14]     --\n",
    "│    └─Sequential (0)                    [64, 1, 28, 28]      [64, 32, 28, 28]     --\n",
    "│    │    └─Conv2d (0)                   [64, 1, 28, 28]      [64, 32, 28, 28]     320\n",
    "│    │    └─ReLU (1)                     [64, 32, 28, 28]     [64, 32, 28, 28]     --\n",
    "│    │    └─Conv2d (2)                   [64, 32, 28, 28]     [64, 32, 28, 28]     9,248\n",
    "│    │    └─ReLU (3)                     [64, 32, 28, 28]     [64, 32, 28, 28]     --\n",
    "│    │    └─Conv2d (4)                   [64, 32, 28, 28]     [64, 32, 28, 28]     9,248\n",
    "│    │    └─ReLU (5)                     [64, 32, 28, 28]     [64, 32, 28, 28]     --\n",
    "│    └─MaxPool2d (1)                     [64, 32, 28, 28]     [64, 32, 14, 14]     --\n",
    "├─Sequential (block2)                    [64, 32, 14, 14]     [64, 64, 7, 7]       --\n",
    "│    └─Sequential (0)                    [64, 32, 14, 14]     [64, 64, 14, 14]     --\n",
    "│    │    └─Conv2d (0)                   [64, 32, 14, 14]     [64, 64, 14, 14]     18,496\n",
    "│    │    └─ReLU (1)                     [64, 64, 14, 14]     [64, 64, 14, 14]     --\n",
    "│    │    └─Conv2d (2)                   [64, 64, 14, 14]     [64, 64, 14, 14]     36,928\n",
    "│    │    └─ReLU (3)                     [64, 64, 14, 14]     [64, 64, 14, 14]     --\n",
    "│    │    └─Conv2d (4)                   [64, 64, 14, 14]     [64, 64, 14, 14]     36,928\n",
    "│    │    └─ReLU (5)                     [64, 64, 14, 14]     [64, 64, 14, 14]     --\n",
    "│    └─MaxPool2d (1)                     [64, 64, 14, 14]     [64, 64, 7, 7]       --\n",
    "├─Sequential (block3)                    [64, 64, 7, 7]       [64, 128, 7, 7]      --\n",
    "│    └─Conv2d (0)                        [64, 64, 7, 7]       [64, 128, 7, 7]      73,856\n",
    "│    └─ReLU (1)                          [64, 128, 7, 7]      [64, 128, 7, 7]      --\n",
    "│    └─Conv2d (2)                        [64, 128, 7, 7]      [64, 128, 7, 7]      147,584\n",
    "│    └─ReLU (3)                          [64, 128, 7, 7]      [64, 128, 7, 7]      --\n",
    "│    └─Conv2d (4)                        [64, 128, 7, 7]      [64, 128, 7, 7]      147,584\n",
    "│    └─ReLU (5)                          [64, 128, 7, 7]      [64, 128, 7, 7]      --\n",
    "├─Linear (fc)                            [64, 6272]           [64, 10]             62,730\n",
    "====================================================================================================\n",
    "Total params: 542,922\n",
    "Trainable params: 542,922\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (Units.GIGABYTES): 3.26\n",
    "====================================================================================================\n",
    "Input size (MB): 0.20\n",
    "Forward/backward pass size (MB): 67.44\n",
    "Params size (MB): 2.17\n",
    "Estimated Total Size (MB): 69.81\n",
    "====================================================================================================\n",
    "```\n",
    "You can, of course, type out all of the individual layers or you can build the repeating structure programatically (we'll see more of that next week - your book does this in Chapter 6 on page 209).  Make a model summary to check your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8b0b",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup the data (5 pts)\n",
    "\n",
    "Load the FashionMNIST dataset.  Normalize with mean 0.2860 and standard deviation 0.3530.  Downsample the train dataset to 10% of its original size to make experimentation quick.  You can use this code for downsampling:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Subset\n",
    "np.random.seed(42)  # use this seed for reproducibility\n",
    "subset_indices = np.random.choice(len(train_dataset), size=int(0.1 * len(train_dataset)), replace=False)\n",
    "train_dataset = Subset(train_dataset, subset_indices)\n",
    "```\n",
    "\n",
    "Use the FashionMNIST test dataset for your `valid_dataset`.\n",
    "\n",
    "For the DataLoaders try batch size 64 to start.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae6a7",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd983",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training with SGD (5 pts)\n",
    "\n",
    "Train your model with Stochastic Gradient Descent.  Track the accuracy metric.  You'll likely need to increase both the learning rate and the number of epochs to see the validation accuracy plateau.  \n",
    "\n",
    "Make sure to instantiate a fresh model to see complete training results. (Although you could resume from a checkpoint as part of your experimentation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf2fb",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45651",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50fd4e",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training with AdamW (5 pts)\n",
    "\n",
    "Now repeat the previous training using AdamW.  You should be able to use the default learning rate of 0.001 and fewer epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0803",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dd4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b50f",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare SGD and AdamW Training Performance\n",
    "\n",
    "Make plots of validation loss and accuracy for both SGD and AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad933",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f305",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Augmentation (5 pts)\n",
    "\n",
    "Now use data augmentation.  Build a transform_train pipleline that includes\n",
    "* Random horizontal flips\n",
    "* Random crops of size 28, padding = 4\n",
    "* Random rotations up to 10 degrees\n",
    "\n",
    "Use the same seed to downsample the train_dataset to 10% of its size.\n",
    "\n",
    "In the next cell, set up the data and augmentation transforms (don't augment the validation data).  Build the DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ef1c",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train a new instance of your model with the new DataLoaders and AdamW.  Training will take more epochs so you may have to experiment a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce9b3",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd97",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0904",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Compare validation loss and accuracy for the three different approaches so far: SGD, AdamW, and AdamW with augmentation.  Make approriate graphs and comment on the three training strategies in terms of their performance on metrics and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c4e3",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a086",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "source": [
    "#### Solution\n",
    "REPLACE_WITH_SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Early Stopping (5 pts)\n",
    "\n",
    "Early stopping isn't really necessary unless the metrics on the validation or test set start to degrade.  Try it anyway just to reenforce how it works.  In this section implement early stopping based on the validation loss.  Use AdamW and data augmentation.  Add a comparison plot of the two methods.  Comment on the performance with and without early stopping.  Do you get comparable performance?  Add cells in this section as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08fa51",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f28",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "source": [
    "#### Solution\n",
    "REPLACE_WITH_SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## OneCycleLR (5 pts)\n",
    "\n",
    "Create a new instance of the model.  Implement a OneCycleLR learning rate scheduler and add it your AdamW approach with data augmentation.\n",
    "You should be able to use a larger max learning rate of 0.003 or so.  Experiment a little to see if you can get similar results to the above with few epochs (you may not be able to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e7575",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4b9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the checkpoint file and make graphs showing the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4922c",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make a plot comparing the validation losses and accuracies for all of the training approaches above (there should be 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bc138",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Which approach works best?   Why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9d2",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "source": [
    "#### Solution\n",
    "REPLACE_WITH_SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use best approach on full dataset (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03974b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Take your best approach and apply it to the full dataset.  (Don't downsample)\n",
    "\n",
    "This will take a little more than a minute per epoch so run your experiments with the smaller dataset above, then run this once.  You can use `resume_from_checkpoint = True` if you want to extend the training.\n",
    "\n",
    "How does this compare to the performance you achieved in HW 2.  Import your best run from HW 2 and make a plot comparing the performance of your best approach from this assignment to the approach from the second assignment.  You might need to quickly retrain your HW2 model using the val_loader instead of the test_loader in train_network.\n",
    "\n",
    "Add code and markdown cells below as needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69f18a",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c49e4d",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {},
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
