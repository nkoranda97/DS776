{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68a90",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `introdl` module is already installed.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to ensure course package is installed\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "course_tools_path = Path('../../Lessons/Course_Tools/').resolve() # change this to the local path of the course package\n",
    "sys.path.append(str(course_tools_path))\n",
    "\n",
    "from install_introdl import ensure_introdl_installed\n",
    "ensure_introdl_installed(force_update=False, local_path_pkg= course_tools_path / 'introdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43fa",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# add all imports here\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b439c",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Homework 4 - Deeper Networks\n",
    "\n",
    "In this assignment you'll explore using a deeper, fully connected model to classify FashionMNIST images again.  The point isn't really to get the best classifier possible, rather we want you to get some experience seeing how deep networks can be difficult to train and how using batch normalization and residual connections can make a network easier to train while also increasing the performance.  Along the way you'll practice building a deep model using loops instead of typing out each individual layer.\n",
    "\n",
    "\n",
    "## Part 1 - Build a Deep Fully Connected Network and Train it on Downsample MNIST (8 pts)\n",
    "\n",
    "Our network will consist of:\n",
    "* input layer which maps the flattened 784 pixels to 64 neurons followed by a ReLU function.\n",
    "* 10 blocks, each block has linear + ReLU + linear + ReLU, the number of neurons stays at 64\n",
    "* a linear output layer that maps the 64 neurons to 10 output neurons (for 10 classes) \n",
    "\n",
    "Use this template (cut and paste into a code cell and complete the model)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        # fill in the repeating elements here linear(hidden_dim, hidden_dim) + ReLU + linear + ReLU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # fill the forward methods to call linear + ReLU + linear + ReLU and return result\n",
    "        return # complete\n",
    "\n",
    "class Deep_MNIST_FC(nn.Module):\n",
    "    def __init__(self, num_blocks=10):\n",
    "        super(Deep_MNIST_FC, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = ### complete the input layer with linear and ReLU, will handle flatten in the forward below\n",
    "    \n",
    "        \n",
    "        # Repeating simple blocks\n",
    "        self.blocks = nn.Sequential(*[SimpleBlock(64) for _ in range(num_blocks)])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = ### add output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        ## call input layer\n",
    "        ## call blocks\n",
    "        ## call output layer\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the downsampled (10% of training data) FashionMNIST DataSet and DataLoaders here.  Include the data augmentation from last week.  Use batchsize = 64.  If you're not doing so already, instead of test_dataset and test_loader make the second dataset (with train = False) the valid_dataset and the loader should be valid_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model with AdamW (lr = 0.001) for 40 epochs.  Track the accuracy.  Be sure to save a checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0803",
   "metadata": {
    "collapsed": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use plot_training_metrics to plot the loss and accuracy for the training and validation sets.  You don't have to display the results dataframe, just the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Use Batch Normalization (8 pts)\n",
    "\n",
    "Create a new model class called Deep_MNIST_FN based on your code from Part 1.  You should now use BatchNorm1d after every linear layer.  Create an instance of your model and train it for 40 epochs.  Produce the same graphs.  Comment on how this model trained compared to the model in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Use Residual Connections (8 pts)\n",
    "\n",
    "Now create a new model class called Deep_MNIST_FC_Res by modifying the model from Part 1 so that each block (with two linear layers) has a residual connection around that block.  This model should not include batch normalization.  Train for 40 epochs.  Make plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Use Residual Connections and Batch Normalization (8 pts)\n",
    "\n",
    "Now create a new model class called Deep_MNIST_FC_Res_BN by modifying one of the models from above so that each block (with two linear layers) has a residual connection around that block and all linear layers are followed by batch normalization.  Train for 40 epochs.  Make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Analysis and Comparison (8 pts)\n",
    "\n",
    "Create two plot showing the validation loss and validation accuracy for each of the four models above.  Write a comparison of the four models.  If you could only choose batch normalization or residual connections, which would it be?  Does the model with both residual connections and batch normalization perform better than the others.  Which approach yeilds the fastest training? Do you find that both residual connections and batch normalization are necessary?  Address ALL of these questions in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
