{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "Watch this video to learn about the course tools package and a bit about how these notebooks are structures.  You'll also see what to do if you find that packages are missing in CoCalc.|\n",
    "\n",
    "NOTE: the last link is working even if the first one does not.  I'm working on it ...\n",
    "\n",
    "#### L01_2_Course_Tools_Missing_Packages Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_course_tools_missing_packages/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_course_tools_missing_packages/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/7hmQDHyrF6V\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force update requested. Uninstalling `introdl`...\n",
      "Found existing installation: introdl 1.0\n",
      "Uninstalling introdl-1.0:\n",
      "  Successfully uninstalled introdl-1.0\n",
      "Installing `introdl` from local directory: /Users/nick/projects/DS776/Lessons/Course_Tools/introdl\n",
      "Processing /Users/nick/projects/DS776/Lessons/Course_Tools/introdl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: IPython in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (8.29.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (8.1.5)\n",
      "Requirement already satisfied: ipycanvas in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (0.13.3)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (3.7.5)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (2.1.4)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (1.11.4)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (0.13.2)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (2.6.0)\n",
      "Requirement already satisfied: torchinfo in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (1.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (0.21.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from introdl==1.0) (4.66.5)\n",
      "Requirement already satisfied: pillow>=6.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from ipycanvas->introdl==1.0) (10.4.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from ipywidgets->introdl==1.0) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from ipywidgets->introdl==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from ipywidgets->introdl==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from ipywidgets->introdl==1.0) (3.0.13)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from IPython->introdl==1.0) (4.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from matplotlib->introdl==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas->introdl==1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pandas->introdl==1.0) (2024.2)\n",
      "Requirement already satisfied: filelock in /Users/nick/.local/lib/python3.12/site-packages (from torch->introdl==1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from torch->introdl==1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from torch->introdl==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from torch->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from torch->introdl==1.0) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from torch->introdl==1.0) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from torch->introdl==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from sympy==1.13.1->torch->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from jedi>=0.16->IPython->introdl==1.0) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from pexpect>4.3->IPython->introdl==1.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->introdl==1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->introdl==1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from jinja2->torch->introdl==1.0) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from stack-data->IPython->introdl==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from stack-data->IPython->introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages (from stack-data->IPython->introdl==1.0) (0.2.3)\n",
      "Building wheels for collected packages: introdl\n",
      "  Building wheel for introdl (pyproject.toml): started\n",
      "  Building wheel for introdl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for introdl: filename=introdl-1.0-py3-none-any.whl size=37141 sha256=d60ea0bf637487d6c5781753875651e6b6675d00a8bdcf5e3daefa97991068ed\n",
      "  Stored in directory: /private/var/folders/2x/tcwyrc3n4sbgncgpktwvw_f80000gn/T/pip-ephem-wheel-cache-ajoxift1/wheels/b2/d9/5e/b96dd95916d1043dc59b77e448879150615f9b650f3e702643\n",
      "Successfully built introdl\n",
      "Installing collected packages: introdl\n",
      "Successfully installed introdl-1.0\n",
      "The `introdl` module is now installed.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to ensure course package is installed\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "course_tools_path = Path('../Course_Tools/').resolve() # change this to the local path of the course package\n",
    "sys.path.append(str(course_tools_path))\n",
    "\n",
    "from install_introdl import ensure_introdl_installed\n",
    "ensure_introdl_installed(force_update=False, local_path_pkg= course_tools_path / 'introdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d43fa",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=.\n",
      "DATA_PATH=.\n",
      "TORCH_HOME=.\n",
      "HF_HOME=.\n"
     ]
    }
   ],
   "source": [
    "# imports and configuration\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from introdl.utils import get_device, load_results, load_model, config_paths_keys\n",
    "from introdl.idlmam import train_simple_network\n",
    "from introdl.visul import plot_training_metrics\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [8, 6]  # Set the default figure size (width, height) in inches\n",
    "\n",
    "paths = config_paths_keys()\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "DATA_PATH = paths['DATA_PATH']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f413",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nonlinear Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a7b",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Curve Fitting Activity\n",
    "\n",
    "The folks at Carnegie Mellon University maintain a version of the Neural Network Playground for one-dimensional curve fitting and classification problems. \n",
    "\n",
    "You should open this <a href=\"https://www.cs.cmu.edu/~pvirtue/tfp/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-saw&learningRate=0.03&regularizationRate=0&noise=50&networkShape=4&seed=0.50727&showTestData=false&discretize=false&percTrainData=50&x=true&collectStats=false&problem=regression&initZero=false&hideText=false\" target=\"_blank\">pre-configured version</a> and tinker with the app to answer the the following questions about using a neural network to fit the one-dimensional \"M\" curve. To play with the app, you should focus mostly on changing the structure of the network.  You can train the network by pressing the \"play\" button.\n",
    "\n",
    "1. Does a network with a single hidden layer with 4 neurons reliably fit the curve?  Reliably means that it nearly always converges to the correct curve when the network is retrained.  Small networks will often get stuck in local minima that don't fit the data well.\n",
    "2. Try two hidden layers.  Is it reliable now?  How many neurons do you need?  Does it converge more quickly if the layers are larger?\n",
    "3. By adding more neurons and layers can you better predict the \"M\" shape and make training more reliable?\n",
    "4. Try reducing the \"Ratio of training to test data\" to 10%.  Now fit a complex network to the data.  What do you notice about the curve?  Is it getting the general pattern or is memorizing the wiggles in the small training set?\n",
    "\n",
    "<details> \n",
    "<summary>AFTER TRYING ON YOUR OWN CLICK HERE TO REVEAL OUR ANSWERS.</summary>\n",
    "\n",
    "1. This small network will have trouble fitting the curve.  This small network can fit the \"M\"-curve quite well, but the training will usually get stuck in a way that the network misses one or both peaks.\n",
    "2. With 6 neurons and 2 neurons, the network will sometimes fit the M well.  With 6 and 6 it seems to nearly always fit the curve well, though it might be a little slow to converge.  With 8 and 8 it seems to always fit well and converge quickly.\n",
    "3. Increasing the number of hidden layers and neurons even further doesn't seem to help much.  The fit isn't any better and the rate of convergence is the same.\n",
    "4. We tried 4 hidden layers with eight neurons each.  The network seems to be trying to fit the wiggles in the data instead of the overall \"M\"-shaped pattern.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c3e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A Note on Splitting the Data\n",
    "\n",
    "In the first couple of lessons we're going to use only a **train-test splits** which is fine for quick prototyping or if you're using cross-validation on the training set. If the dataset is small to begin with, then using a **train-validation-test** split  may not be possible because there simply isn't have enough data.  Cross-validation is used in some research papers, but it is rare in real-world deep learning workflows because training models is resource and time intensive.\n",
    "\n",
    "If you have a large dataset you should nearly always use a **train-validation-test** split.  Most of the time in deep learning we need to adjust hyperparameters, like the learning rate and batch size, and we want to try various model architectures and optimizers. If all of these variations are done to the test set then we risk overfitting on that set with no means to assess the performance of our final deep learning pipeline on unseen data.\n",
    "\n",
    "With a **train-validation-test** split the setup is:\n",
    "  - The **training set** is used to train the model.\n",
    "  - The **validation set** is used to tune hyperparameters, select models, and monitor for overfitting.\n",
    "  - The **test set** is used to evaluate final model performance and should only be used **once**, after model selection and tuning, to assess generalization to unseen data.\n",
    "\n",
    "Validation sets are critical for:\n",
    "\n",
    "- **Hyperparameter optimization**: Avoiding overfitting on the test set.\n",
    "- **Model comparison**: Selecting the best-performing model based on validation metrics.\n",
    "- **Early stopping**: Using validation loss to decide when to stop training.\n",
    "\n",
    "If you use only a test set for hyperparameter tuning and model selection, you risk overfitting to the test data, making the test results less meaningful. Using a validation set ensures the test set remains untouched for final performance evaluation.\n",
    "\n",
    "We'll introduce train-validation-test splits in Lesson 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Model in PyTorch\n",
    "\n",
    "Now we'll follow the steps below to implement and train a similar model in PyTorch.  Based on our experiments in the activity above we'll use a fully-connected neural network with two hidden layers each containing eight neurons.\n",
    "\n",
    "1. Prepare the Data\n",
    "\n",
    "2. Configure the Model\n",
    "\n",
    "3. Train the Model\n",
    "\n",
    "4. Evaluate the Model\n",
    "\n",
    "5. Make Predictions\n",
    "\n",
    "Those five steps are common to any machine learning workflow.  Often we'll repeat some of those steps, particularly Step 2-4, multiple times until we're satisfied with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf37c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 1 - Prepare the Data\n",
    "\n",
    "#### L01_2_prepare_the_data Video\n",
    "\n",
    "Here is a video walkthrough for this step.\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_prepare_the_data/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_prepare_the_data/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/6mXJjH6NFJ8\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "<hr>\n",
    "\n",
    "For this example we'll first create synthetic data then we'll use PyTorch Datasets and DataLoaders to get in the form we'll use for  model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04302d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Create Synthetic Data\n",
    "\n",
    "We'll generate two sets of 400 points each for training and testing.  You can dig through the code below if you want details about how we're generating the data, but in short:\n",
    "* Generate a bunch of $x$-values.  We're using a uniform distribution to generate these.\n",
    "* Each $y$-value is computed by applying a formula $f(x)$ and adding normally distributed random noise.\n",
    "\n",
    "At the end of this we want our neural network to be a reasonable approximation to $f(x)$.  Note that normally distributed random noise has mean 0 and standard deviation of 0.5.  This means that most of the $y$-values are within $\\pm 0.5$ of the underlying true $y$-values.  We'll recall this fact later when we try to assess the quality of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b925",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42); # make results reproducible\n",
    "\n",
    "def MCurve(x):\n",
    "    \"\"\"\n",
    "    The \"M\" curve function.\n",
    "\n",
    "    Parameters:\n",
    "    - x (torch.Tensor): The input values.\n",
    "\n",
    "    Returns:\n",
    "    - y (torch.Tensor): The output values.\n",
    "    \"\"\"\n",
    "    return 3 * F.relu(x + 6) - 6 * F.relu(x + 4) + 6 * F.relu(x) - 6 * F.relu(x - 4)  # f(x) is an \"M\" curve\n",
    "\n",
    "def makeMCurveData(num_samples, std_dev=0.5):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the \"M\" curve.\n",
    "\n",
    "    Parameters:\n",
    "    - num_samples (int): The number of data points to generate.\n",
    "    - std_dev (float): The standard deviation of the noise added to the data.\n",
    "\n",
    "    Returns:\n",
    "    - x (torch.Tensor): The input values.\n",
    "    - y (torch.Tensor): The output values with added noise.\n",
    "    \"\"\"\n",
    "    x = torch.rand(num_samples) * 12 - 6\n",
    "    y = MCurve(x)\n",
    "    noise = std_dev * torch.randn(num_samples)  # normally distributed noise\n",
    "    return x, y + noise\n",
    "\n",
    "n = 1000\n",
    "x_train, y_train = makeMCurveData(n) # roughly the same as in NN Playground\n",
    "x_test, y_test = makeMCurveData(n) \n",
    "\n",
    "x_grid = torch.linspace(-6,6,201)\n",
    "y_true = MCurve(x_grid)\n",
    "\n",
    "sns.scatterplot(x = x_train, y = y_train, s = 10, legend = False, color='lightsteelblue');\n",
    "sns.lineplot(x=x_grid, y=y_true, color='orange')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the orange \"M-curve\" as the ground truth that we are trying to estimate from noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d38",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create Pytorch Datasets\n",
    "\n",
    "First we need to adjust the shapes of our data tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb111a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50247d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pytorch models can process multiple inputs simultaneously but we'll need to make our inputs into a nested tensor to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b98",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reshape\n",
    "\n",
    "We want our input tensor to have shape `[batch_size, num_features]` or `[1000,1]` in this case.\n",
    "\n",
    "Here's an example to show how `reshape` can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff31",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4])\n",
    "print('1D tensor')\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print('\\n2D tensor')\n",
    "x2 = x.reshape(-1,1) # make [N,1] shape where N is the number of elements in x\n",
    "print(x2)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad40",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The training and testing Datasets\n",
    "\n",
    "In PyTorch, a `Dataset` is an abstract class that serves as a blueprint for accessing and processing data.\n",
    "\n",
    "`TensorDataset` is a special `Dataset` class that is useful for small datasets that fit easily in memory in Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "741b90",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train.reshape(-1,1), \n",
    "                              y_train.reshape(-1,1))\n",
    "test_dataset = TensorDataset(x_test.reshape(-1,1), \n",
    "                             y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67937",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### The DataLoaders\n",
    "\n",
    "In PyTorch, a `DataLoader` wraps a `Dataset` to manage batching, shuffling, and parallel data loading, making data feeding efficient during model training and evaluation.\n",
    "\n",
    "Shuffling the training data is important because it helps to prevent the model from learning patterns specific to the order of the data, leading to better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f06b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 50 # we set this the same as in Neural Network playground\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size = batch_size, \n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size = batch_size, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  A sample batch\n",
    "It's a good idea to inspect a batch of the data to see if it's what you expect.  \n",
    "\n",
    "It's also good to note the shape of a batch because we'll use that to shape to make sure our PyTorch neural network model is behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754622",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(xb[:5],'\\n')\n",
    "print(xb.shape,'\\n')\n",
    "print(yb[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4156",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 2 - Configure the Model\n",
    "\n",
    "#### L01_2_configure_the_model Video\n",
    "\n",
    "Here is a video walkthrough for Step 2:\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_configure_the_model/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_configure_the_model/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/ylXSAeD5IaA\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "* 2 hidden layers with 8 neurons each\n",
    "* hyperbolic tangent activation functions\n",
    "* use `torch.nn.Sequential` for simple networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "892d43",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 8),   # First hidden layer (input: 1, output: 8)\n",
    "    nn.Tanh(),         # Tanh activation function\n",
    "    nn.Linear(8, 8),   # Second hidden layer (input: 8, output: 8)\n",
    "    nn.Tanh(),         # Tanh activation function\n",
    "    nn.Linear(8, 1)    # Output layer (input: 8, output: 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a8",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Define the model as a class\n",
    "\n",
    "While the're nothing wrong with using the `Sequential` class to define simple models, it does have a couple of disadvantages.\n",
    "\n",
    "First, if you need to make a new instance of the model you'll have to copy the code.  Second, more complex models will require us to define the models directly as classes that inherit from the `nn.Module` class.  We'll start doing that right away so you get used to it.\n",
    "\n",
    "If you're completely unfamiliar with Object-Oriented Programming in Python I recommend this [Real Python Tutorial](https://realpython.com/python3-object-oriented-programming/).  Written and video versions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CurveFitter(nn.Module):\n",
    "    # The __init__ method in a PyTorch model, often called the constructor, \n",
    "    # is used to initialize the model's layers and any other parameters or \n",
    "    # components needed for the model. This method is called when an instance \n",
    "    # of the model is created.  All the layers that involve model parameters \n",
    "    # must be declared here.  The activation functions can be put here \n",
    "    # or in the forward method as we'll see later.\n",
    "    #\n",
    "    # The super() function is used to call the constructor of the parent class.\n",
    "    def __init__(self):\n",
    "        super(CurveFitter, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 8),   # First hidden layer (input: 1, output: 8)\n",
    "            nn.Tanh(),         # Tanh activation function\n",
    "            nn.Linear(8, 8),   # Second hidden layer (input: 8, output: 8)\n",
    "            nn.Tanh(),         # Tanh activation function\n",
    "            nn.Linear(8, 1)    # Output layer (input: 8, output: 1)\n",
    "        )\n",
    "\n",
    "    # The forward method defines the forward pass of the model.  \n",
    "    # It's called every time we execute the model on input data.\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = CurveFitter()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229ee",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Summarizing a model\n",
    "\n",
    "When you're building PyTorch models or even applying existing models to new data, the number one source of errors is mismatched tensor shapes.  `torchinfo.summary` is great for seeing how the batch of input data changes shape as it moves through the network.  If there is an issue with mismatched sizes, you'll get an error message that shows you the last layer that was able to be executed which is super helpful in diagnosing shape errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47701",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(model, input_size = (40,1), col_width=16,\n",
    "        col_names = [\"input_size\",\"output_size\",\"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8142",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3 - Train the model\n",
    "\n",
    "#### L01_2_train_the_model Video\n",
    "\n",
    "Here is a video walkthrough for Step 3:\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_train_the_model/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_train_the_model/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/P5XcxjuNC2u\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "\n",
    "To setup training:\n",
    "\n",
    "* choose loss function\n",
    "* choose metric(s) if desired\n",
    "* set device to cpu or gpu if available\n",
    "* determine a name for saved model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32b4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mean Square Error Loss\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# Mean Absolute Error metric for monitoring\n",
    "score_funcs = {'MAE':mean_absolute_error}\n",
    "\n",
    "# determine device to run the model on\n",
    "device = get_device() # from course package\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# checkpoint filename for saving model\n",
    "ckpt_file = MODELS_PATH / 'L01_MCurveData_CurveFitter.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f08",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notes:\n",
    "\n",
    "* It isn't required to choose a separate metric that is different from the loss function, but we often want to do so.  In this case Mean absolute error gives a better sense of the average vertical deviation of the fitted curve and a data point while the mean squared error loss can be harder to interpret.\n",
    "\n",
    "* Note, I'm either training on a gaming PC with an Nvidia GPU or a Macbook Pro, so you'll see that my device is either 'cuda' or 'mps'.  In CoCalc and other platforms you'll either use a 'cpu' or 'cuda' if a GPU is available.   We use the get_device function from the course Python package to see what device is available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836ad",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setting the device\n",
    "\n",
    "Just so you can see how it works, here is the code for the 'get_device' function.  We put it in the package so it would be easy to change later if other hardware gets PyTorch support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce3eb6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Returns the appropriate device ('cuda', 'mps', or 'cpu') depending on availability.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Do the training\n",
    "\n",
    "Initially, we'll make use of `train_simple_network` from the textbook.  Use the version from the course package (imported at the top of the notebook) since we've added a couple of tweaks.  Here's the function signature so you can read about the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?train_simple_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our experiments with Neural Network playground we saw that a learning rate of 0.03 worked well and that the training converged in 200-300 epochs.  We'll run for 600 epochs here just to be sure we're converged.  We're also using a larger batch size (50) than in the playground to expedite training.  \n",
    "\n",
    "Note - the training sometimes yields a model that isn't a great fit to the data because the optimization gets stuck in a poor-quality local minimum.  You can rerun the code to see if does better or you can load a model from our checkpoint file.  Later in the course we'll see how to use better optimizers that are less likely to get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2671",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CurveFitter() # instantiate a newly initialized model\n",
    "\n",
    "results_df = train_simple_network(model, \n",
    "                                  loss_func, \n",
    "                                  train_loader, \n",
    "                                  device=device, \n",
    "                                  epochs = 600, \n",
    "                                  lr = 0.03, \n",
    "                                  checkpoint_file=ckpt_file, \n",
    "                                  test_loader=test_loader,\n",
    "                                  score_funcs = score_funcs,\n",
    "                                  use_tqdm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e28",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4 - Evaluate the Model\n",
    "\n",
    "#### L01_2_evaluate_the_model Video\n",
    "\n",
    "Here is a video walkthrough for Step 4:\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_evaluate_the_model/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_evaluate_the_model/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/e6HmrPxo2oR\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "\n",
    "\n",
    "To evaluate the model we are usually interested in a couple of things.\n",
    "1. How well does the model perform on data that the model wasn't trained on?  \n",
    "2. Is the model underfitting or overfitting?  \n",
    "\n",
    "In the world of deep learning rarely is underfitting an issue, but overfitting will be common so we'll keep an eye out for it.\n",
    "\n",
    "Since we're training the model once as a quick prototype we'll just use the test dataset from our train-test split.  By inspecting loss and other metrics on both the train and test set we can assess both the model performance and whether it is overfitting.\n",
    "\n",
    "#### Inspect convergence loss and metrics\n",
    "\n",
    "We could simply refer to `results_df` here to see the loss and metrics, but we'll show how to load it from the checkpoint file.  That's really useful so you don't have to retrain the model just to review the results.  A little later we'll introduce a `load_results` function to simplify loading results from a checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe339",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ckpt_file = MODELS_PATH / 'L01_MCurveData_CurveFitter.pt' # load parameters from checkpoint\n",
    "checkpoint_dict = torch.load(ckpt_file, weights_only=False, map_location=torch.device('cpu')) \n",
    "results_df = pd.DataFrame(checkpoint_dict['results']) # convert results to DataFrame\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to tell from the mean squared error if our model is doing a good job of fitting the data, but look at the mean absolute error.  It converges to a value around 0.5 for both the testing and training data.  This means that, on average, the predicted $y$_values are within $\\pm 0.5$ of target $y$-values.  Recall that the standard deviation of noise in the target $y$-values was 0.5.  Since the errors in predicting the target values are similar to the magnitude of the noise in the original target values, we can't really do any better than that!\n",
    "\n",
    "You can also see that both MSE and MAE are converging pretty similarly for both the test and training sets.  This is exactly what we'd like to see.  This means the model is neither underfitting or overfitting.\n",
    "\n",
    "(Truth in advertising - this particular model and training combination often gets stuck in a local minimum that doesn't fit the data as well as it could similar to what happens in Neural Network Playground.  These issues could be addressed by introducing a more complex model and improving the training.  We'll see more about improved training in Lesson 3.  This version of the model still serves as good introduction to a deep learning workflow in PyTorch.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4a8",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspect convergence graphically\n",
    "\n",
    "It's often more helpful to see the training metrics visually instead of looking at the raw values.\n",
    "\n",
    "In the next cell we show you how to plot the metrics using `seaborn.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bae",
   "metadata": {
    "collapsed": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10,3) )\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "sns.lineplot(x='epoch',y='train loss',data=results_df,label='Train', ax=axes[0]);\n",
    "sns.lineplot(x='epoch',y='test loss',data=results_df,label='Test', ax=axes[0]);\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('MSE Loss vs Epoch');\n",
    "\n",
    "sns.lineplot(x='epoch',y='train MAE',data=results_df,label='Train', ax=axes[1]);\n",
    "sns.lineplot(x='epoch',y='test MAE',data=results_df,label='Test', ax=axes[1]);\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE vs Epoch');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could use `plot_training_metrics` (an interactive version is also available `plot_training_metrics_widget`).  The `results_df` is probably still in memory, but we'll also show you how to `load_results` to get it here.  These functions are available in the course `introdl` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = load_results(MODELS_PATH / 'L01_MCurveData_CurveFitter.pt') # load results from checkpoint and returns data frame\n",
    "\n",
    "plot_training_metrics(results_df,[['train loss', 'test loss'],['train MAE', 'test MAE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e295",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Visualize the fit\n",
    "\n",
    "We can already see from the mean absolute error on the test data that our trained model is doing a good job, but for this curve-fitting problem it's relatively easy to visualize how well the trained network fits the data.  First we'll create a new instance of the model and overwrite its randomly initialized parameter values with the saved parameters from our checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76162d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CurveFitter() # new instance of model\n",
    "\n",
    "ckpt_file = MODELS_PATH / 'L01_MCurveData_CurveFitter.pt' # load parameters from checkpoint\n",
    "checkpoint_dict = torch.load(ckpt_file, weights_only=False, map_location=torch.device('cpu')) \n",
    "model.load_state_dict(checkpoint_dict['model_state_dict']); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading models from checkpoint files is something we'll do often so to streamline this process we've provided a `load_model` function in the `introdl` package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can either pass in the model instance or the class\n",
    "model = load_model(CurveFitter, MODELS_PATH / 'L01_MCurveData_CurveFitter.pt')\n",
    "\n",
    "# this would also work\n",
    "# model = CurveFitter() # create a new instance of the model\n",
    "# model = load_model(model, MODELS_PATH / 'L01_MCurveData_CurveFitter.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the model on a sequence of equally spaced inputs for visualization.  We'll discuss the code below in the next section, when we discuss using a model to make predictions for new data, but here's the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc733c",
   "metadata": {
    "collapsed": false,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-6,6,201).reshape(-1,1) # add a batch dimension\n",
    "model.to(torch.device('cpu')) # move model to CPU if necessary\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "y_true = MCurve(x)\n",
    "\n",
    "# Add the scatter plot for noisy y-values\n",
    "sns.scatterplot(x=x_test, y=y_test, s=10, color = \"lightsteelblue\") # blue\n",
    "sns.lineplot(x=x.squeeze(),y=y.squeeze(),color=\"#55A868\",label=\"Model\") # green\n",
    "sns.lineplot(x=x.squeeze(),y=y_true.squeeze(),color=\"orange\",label=\"True\") # orange\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model often gets stuck in the wrong local minimum so the fit may not be great.  In later lessons we'll study more effective training techniques that reduce the chance of getting stuck in poor local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Make Predictions\n",
    "\n",
    "#### L01_2_make_predictions Video\n",
    "\n",
    "Here is a video walkthrough for Step 5:\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_make_the_predictions/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l01_2_make_the_predictions/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/P25g0EleiQH\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "We just did this in the last code cell, but let's look at how we did it.  To use a trained model to make predictions using new inputs requires a bit of preparation.  There are a few things to do:\n",
    "\n",
    "1.  Prepare the new input data for the model.  This usually means converting to a tensor with the appropriate shape.\n",
    "2.  Load the model and put in evaluation mode so that PyTorch knows to not compute gradients.\n",
    "3.  Make the sure the model and data are on the same device.\n",
    "4.  If necessary, convert the raw model outputs into the predictions you want and convert those predictions into the data type or structure you need.\n",
    "\n",
    "Let's tackle this writing a function to do most of these steps.  You may need to build a similar function for your application or problem.  First decide what formats you want for the input and output data.  In this example we'll use PyTorch tensors, but you might want Numpy arrays, lists, or something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions from PyTorch Tensors\n",
    "\n",
    "Here we're assuming that that $x$-values (the features) where you want to evaluate the model are in a PyTorch tensor with shape $[N,1]$.  Where $N$ is the number of $x$-values and the 1 is because each feature is a single number.  The data and model are likely to be on the 'cpu' already, but it doesn't hurt to make sure.  You may want to use a GPU if one is available for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1d(model, x, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Predict the output of a model for input value(s).\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model.\n",
    "    - x (torch.Tensor): The input value(s). Must have a shape of (N,1).\n",
    "    - device (torch.device): The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "    - y (torch.Tensor): The model's output. Will have shape (N,1).\n",
    "    \"\"\"\n",
    "    model.to(device) # move model to device\n",
    "    model.eval() # set model to evaluation mode\n",
    "    x = x.to(device) # move input to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y = model(x.float()) # make predictions for input value(s)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make predictions for a few arbitrary points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4]).reshape(-1,1)\n",
    "y = predict1d(model,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions from Numpy Arrays\n",
    "\n",
    "A more typical use case might be to make predictions for values in a Numpy array.  We'll make a new function that converts the numpy array to a PyTorch tensor, applies the model, then converts the output back to a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict1d_numpy(model, x, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Predict the output of a model for input value(s).\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model.\n",
    "    - x (numpy.ndarray): The input value(s). Must have a shape of (N,).\n",
    "    - device (torch.device): The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "    - y (numpy.ndarray): The model's output. Will have shape (N,).\n",
    "    \"\"\"\n",
    "    x_tensor = torch.from_numpy(x).reshape(-1, 1)\n",
    "    y_tensor = predict1d(model, x_tensor, device)\n",
    "    y = np.squeeze(y_tensor.cpu().numpy()) # move to CPU and convert to numpy, squeeze removes dimensions of size 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our function to make predictions and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6,6,201)\n",
    "y = predict1d_numpy(model,x)\n",
    "plt.plot(x,y,label='Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
