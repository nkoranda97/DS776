{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "text = (\n",
    "    \"Hello TechEmporium, I recently purchased a set of noise-canceling headphones \"\n",
    "    \"from your online store in Canada. Upon receiving the item, I noticed a defect: \"\n",
    "    \"the left earcup doesn’t produce sound. As someone who relies on quality audio \"\n",
    "    \"for my work, this is a major inconvenience. I would appreciate a replacement or \"\n",
    "    \"a refund. Attached are the order details and proof of purchase. Please let me \"\n",
    "    \"know how we can resolve this issue promptly. Best regards, Alex.\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size:\n",
      "66,955,010 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9983344674110413}]\n",
      "\n",
      "**Named Entity Recognition**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889\n",
      "parameters\n",
      "[{'entity_group': 'MISC', 'score': 0.9910073, 'word': 'Samsung Galaxy S24\n",
      "Ultra', 'start': 14, 'end': 38}, {'entity_group': 'ORG', 'score': 0.99012053,\n",
      "'word': 'Tech Haven', 'start': 44, 'end': 54}, {'entity_group': 'MISC', 'score':\n",
      "0.992503, 'word': 'Google Pixel 8 Pro', 'start': 325, 'end': 343},\n",
      "{'entity_group': 'ORG', 'score': 0.96618426, 'word': 'Tech Haven', 'start': 551,\n",
      "'end': 561}, {'entity_group': 'PER', 'score': 0.98039377, 'word': 'Jamie',\n",
      "'start': 597, 'end': 602}]\n",
      "\n",
      "**Question Answering**\n",
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450\n",
      "parameters\n",
      "{'score': 0.11689740419387817, 'start': 222, 'end': 233, 'answer': 'stock\n",
      "issue'}\n",
      "\n",
      "**Translation**\n",
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Pedí el Samsung Galaxy S24 Ultra de Tech Haven, esperando\n",
      "la entrega del día siguiente, pero después de tres días, ni siquiera había\n",
      "recibido una actualización de envío. Después de esperar 45 minutos en espera, el\n",
      "servicio al cliente me dijo que había un problema de existencias — sin embargo,\n",
      "nadie me había informado! Cuando el paquete finalmente llegó una semana tarde,\n",
      "que contenía un Google Pixel 8 Pro en su lugar. El representante de apoyo era\n",
      "apologético, pero dijo que un intercambio tomaría otras dos semanas. Pagué\n",
      "$1,200 por el teléfono equivocado, trató con retrasos y mala comunicación, y\n",
      "ahora tienen que esperar aún más. Tech Haven, usted necesita hacer mejor!\n",
      "Sinceramente, Jamie.'}]\n",
      "\n",
      "**Summarization**\n",
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late, it contained a\n",
      "Google Pixel 8 Pro instead . An exchange would take another two weeks .'}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoModelForQuestionAnswering, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from introdl.utils import get_device, wrap_print_text\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "# get GPU if available\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadn’t even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issue—yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "def print_model_info(pipe):\n",
    "    model = pipe.model\n",
    "    model_size = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {model.name_or_path}, Size: {model_size:,} parameters\")\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_model_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\")\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True, device=device)\n",
    "print_model_info(ner_pipeline)\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n",
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_model_info(qa_pipeline)\n",
    "question = \"What is the defect?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n",
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_model_info(translation_pipeline)\n",
    "translation_result = translation_pipeline(text, max_length=200)\n",
    "print(translation_result)\n",
    "\n",
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=device)\n",
    "print_model_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cache for local models\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "def llm_prompt(prompt, model_str, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate a response from an LLM based on the given model string.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input text prompt.\n",
    "        - model_str (str): The model identifier (e.g., 'gpt-4o' or 'meta-llama/Llama-3.2-3B-Instruct').\n",
    "        - max_length (int, optional): Maximum response length. Default is 256.\n",
    "    \n",
    "    Returns:\n",
    "        - str: The model-generated response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Case 1: OpenAI API Model\n",
    "    if model_str in [\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY in the environment.\")\n",
    "        \n",
    "        openai.api_key = api_key\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_str,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_length,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"OpenAI API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model\n",
    "    else:\n",
    "        if model_str not in MODEL_CACHE:\n",
    "            try:\n",
    "                print(f\"Loading model: {model_str} (this may take a while)...\")\n",
    "                MODEL_CACHE[model_str] = {\n",
    "                    \"model\": AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=torch.float16, device_map=\"auto\"),\n",
    "                    \"tokenizer\": AutoTokenizer.from_pretrained(model_str)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return f\"Error loading model {model_str}: {str(e)}\"\n",
    "        \n",
    "        model = MODEL_CACHE[model_str][\"model\"]\n",
    "        tokenizer = MODEL_CACHE[model_str][\"tokenizer\"]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_length=max_length, temperature=0.7)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI GPT-4o with a lower temperature for more deterministic responses\n",
    "response = llm_prompt(\"Explain quantum entanglement.\", \"gpt-4o\", temperature=0.3)\n",
    "print(response)\n",
    "\n",
    "# Using LLaMA with a higher temperature for more creative output\n",
    "response = llm_prompt(\"Tell me a sci-fi story.\", \"meta-llama/Llama-3.2-3B-Instruct\", temperature=1.2)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've updated your `llm_prompt` function to include **search strategies** for local Hugging Face models, giving you control over how responses are generated. You can now specify **different decoding strategies** using a new parameter:  \n",
    "- **`search_strategy=\"greedy\"`** (default) – simple, deterministic response  \n",
    "- **`search_strategy=\"beam\"`** – beam search for better fluency  \n",
    "- **`search_strategy=\"top_k\"`** – random sampling from the top-k most likely tokens  \n",
    "- **`search_strategy=\"top_p\"`** – nucleus sampling for diverse responses  \n",
    "- **`search_strategy=\"contrastive\"`** – contrastive search for high-quality output  \n",
    "\n",
    "---\n",
    "\n",
    "### **Updated `llm_prompt` Function**\n",
    "```python\n",
    "import os\n",
    "import openai\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cache for local models to avoid reloading\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "def llm_prompt(prompt, model_str, max_length=256, temperature=0.7, search_strategy=\"greedy\", top_k=50, top_p=0.9, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generate a response from an LLM based on the given model string.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input text prompt.\n",
    "        - model_str (str): The model identifier (e.g., 'gpt-4o' or 'meta-llama/Llama-3.2-3B-Instruct').\n",
    "        - max_length (int, optional): Maximum response length. Default is 256.\n",
    "        - temperature (float, optional): Controls randomness (0.0 = deterministic, 1.0+ = creative). Default is 0.7.\n",
    "        - search_strategy (str, optional): Decoding strategy. Options: 'greedy', 'beam', 'top_k', 'top_p', 'contrastive'. Default is 'greedy'.\n",
    "        - top_k (int, optional): Used if `search_strategy=\"top_k\"`. Default is 50.\n",
    "        - top_p (float, optional): Used if `search_strategy=\"top_p\"`. Default is 0.9.\n",
    "        - num_beams (int, optional): Used if `search_strategy=\"beam\"`. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        - str: The model-generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Case 1: OpenAI API Model\n",
    "    if model_str in [\"gpt-4o\", \"gpt-4o-mini\", \"o1\", \"o1-mini\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY in the environment.\")\n",
    "\n",
    "        openai.api_key = api_key\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_str,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_length,\n",
    "                temperature=temperature  # Adjustable temperature\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except Exception as e:\n",
    "            return f\"OpenAI API error: {str(e)}\"\n",
    "\n",
    "    # Case 2: Local Hugging Face Model (e.g., LLaMA)\n",
    "    else:\n",
    "        if model_str not in MODEL_CACHE:\n",
    "            try:\n",
    "                print(f\"Loading model: {model_str} (this may take a while)...\")\n",
    "                MODEL_CACHE[model_str] = {\n",
    "                    \"model\": AutoModelForCausalLM.from_pretrained(model_str, torch_dtype=torch.float16, device_map=\"auto\"),\n",
    "                    \"tokenizer\": AutoTokenizer.from_pretrained(model_str)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return f\"Error loading model {model_str}: {str(e)}\"\n",
    "        \n",
    "        model = MODEL_CACHE[model_str][\"model\"]\n",
    "        tokenizer = MODEL_CACHE[model_str][\"tokenizer\"]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Apply search strategy\n",
    "        gen_kwargs = {\"max_length\": max_length, \"temperature\": temperature}\n",
    "\n",
    "        if search_strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False  # Greedy decoding\n",
    "        elif search_strategy == \"beam\":\n",
    "            gen_kwargs[\"num_beams\"] = num_beams  # Beam search\n",
    "        elif search_strategy == \"top_k\":\n",
    "            gen_kwargs.update({\"do_sample\": True, \"top_k\": top_k})  # Top-k sampling\n",
    "        elif search_strategy == \"top_p\":\n",
    "            gen_kwargs.update({\"do_sample\": True, \"top_p\": top_p})  # Nucleus sampling\n",
    "        elif search_strategy == \"contrastive\":\n",
    "            gen_kwargs.update({\"penalty_alpha\": 0.6, \"top_k\": 4})  # Contrastive search\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **New Features & Search Strategies**\n",
    "| Search Strategy  | How It Works | Best Used For |\n",
    "|------------------|-------------|--------------|\n",
    "| **greedy** (default) | Picks the most likely word at each step. | Fast, deterministic output. |\n",
    "| **beam** | Considers multiple possible sequences and picks the best. | Producing high-quality, fluent text. |\n",
    "| **top_k** | Samples from the top-k most likely words. | Adding variety while staying relevant. |\n",
    "| **top_p** | Samples dynamically from the most probable subset of words. | More diverse, natural text. |\n",
    "| **contrastive** | Uses **contrastive search** to balance diversity and quality. | High-quality open-ended generation. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Usage**\n",
    "#### **1️⃣ OpenAI GPT-4o (API)**\n",
    "```python\n",
    "response = llm_prompt(\"Explain quantum mechanics.\", \"gpt-4o\", temperature=0.3)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **2️⃣ LLaMA Using Greedy Decoding**\n",
    "```python\n",
    "response = llm_prompt(\"Tell me a story.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"greedy\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **3️⃣ LLaMA Using Beam Search for Higher Fluency**\n",
    "```python\n",
    "response = llm_prompt(\"Summarize this article.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"beam\", num_beams=5)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **4️⃣ LLaMA Using Top-k Sampling for More Variety**\n",
    "```python\n",
    "response = llm_prompt(\"Write a poem about AI.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"top_k\", top_k=40)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "#### **5️⃣ LLaMA Using Contrastive Search for High-Quality Responses**\n",
    "```python\n",
    "response = llm_prompt(\"Describe the future of AI.\", \"meta-llama/Llama-3.2-3B-Instruct\", search_strategy=\"contrastive\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Update is Useful**\n",
    "✅ **More Control** – Choose between fluency (beam), diversity (top-p), or determinism (greedy).  \n",
    "✅ **Better Text Quality** – Contrastive search improves coherence.  \n",
    "✅ **Preserves API Flexibility** – GPT models still work the same way.  \n",
    "✅ **Fast & Efficient** – **Caches** the model for reuse instead of reloading it each time.  \n",
    "\n",
    "Would you like any additional customization, such as **temperature-dependent sampling adjustments** or **log probabilities** for debugging? 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
