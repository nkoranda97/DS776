{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "def wrap_print_text(print):\n",
    "    \"\"\"Adapted from: https://stackoverflow.com/questions/27621655/how-to-overload-print-function-to-expand-its-functionality/27621927\"\"\"\n",
    "\n",
    "    def wrapped_func(text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        wrapper = TextWrapper(\n",
    "            width=80,\n",
    "            break_long_words=True,\n",
    "            break_on_hyphens=False,\n",
    "            replace_whitespace=False,\n",
    "        )\n",
    "        return print(\"\\n\".join(wrapper.fill(line) for line in text.split(\"\\n\")))\n",
    "\n",
    "    return wrapped_func\n",
    "\n",
    "# Wrap the print function\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of models currently available through the OpenAI API run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "o1-mini\n",
      "o1-mini-2024-09-12\n",
      "o1-preview\n",
      "o1-preview-2024-09-12\n",
      "omni-moderation-2024-09-26\n",
      "omni-moderation-latest\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Fetch the list of models\n",
    "list_of_models = client.models.list()\n",
    "\n",
    "# Extract model IDs and sort them alphabetically\n",
    "model_ids = sorted(model.id for model in list_of_models)\n",
    "\n",
    "# Print the model IDs\n",
    "for model_id in model_ids:\n",
    "    print(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "LOADED_MODELS = {}  # Track loaded models\n",
    "LOADED_TOKENIZERS = {}  # Track tokenizers\n",
    "\n",
    "def release_model():\n",
    "    \"\"\"Unloads the currently loaded model from memory and GPU.\"\"\"\n",
    "    global LOADED_MODELS, LOADED_TOKENIZERS\n",
    "\n",
    "    if LOADED_MODELS:\n",
    "        model_str = next(iter(LOADED_MODELS))  # Get the first (and only) loaded model\n",
    "        print(f\"üõë Unloading model: {model_str} from GPU...\")\n",
    "\n",
    "        # Move model to CPU before deletion\n",
    "        model = LOADED_MODELS[model_str]\n",
    "        model.to(\"cpu\")  # Moves model out of GPU memory\n",
    "        \n",
    "        del LOADED_MODELS[model_str]  # Remove model from registry\n",
    "        del LOADED_TOKENIZERS[model_str]  # Remove tokenizer from registry\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()  # Force garbage collection\n",
    "        print(f\"‚úÖ Model {model_str} has been fully unloaded.\")\n",
    "    else:\n",
    "        print(f\"‚ùå No model to unload.\")\n",
    "\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration object to store model, tokenizer, and API client settings.\"\"\"\n",
    "    def __init__(self, model_str, model=None, tokenizer=None, api_type=None, cost_per_M_input=None, cost_per_M_output=None):\n",
    "        self.model_str = model_str\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.api_type = api_type  # Identifies API model type\n",
    "        self.client = None  # API client instance (if applicable)\n",
    "        self.cost_per_M_input = cost_per_M_input  # Cost per million input tokens\n",
    "        self.cost_per_M_output = cost_per_M_output  # Cost per million output tokens\n",
    "\n",
    "        # Initialize API client if applicable\n",
    "        if api_type in [\"openai\", \"gemini\"]:\n",
    "            api_key = os.getenv(\"GEMINI_API_KEY\" if api_type == \"gemini\" else \"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(f\"Missing {api_type.upper()} API key. Set the appropriate environment variable.\")\n",
    "            \n",
    "            base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\" if api_type == \"gemini\" else None\n",
    "            self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    def unload(self):\n",
    "        \"\"\"Unloads the model and tokenizer from memory and GPU.\"\"\"\n",
    "        release_model()\n",
    "\n",
    "def llm_configure(model_str, cost_per_M_input=None, cost_per_M_output=None):\n",
    "    \"\"\"Ensures only one model is loaded at a time: reuses existing or unloads the previous one.\"\"\"\n",
    "    \n",
    "    if model_str in [\"gpt-4o\", \"gpt-4o-mini\", \"o1-mini\", \"o3-mini\"]:\n",
    "        return ModelConfig(model_str, api_type=\"openai\", cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)\n",
    "\n",
    "    if model_str in [\"gemini-2.0-flash-lite-preview-02-05\", \"gemini-2.0-flash\"]:\n",
    "        return ModelConfig(model_str, api_type=\"gemini\", cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)\n",
    "\n",
    "    global LOADED_MODELS\n",
    "\n",
    "    # Check if a model is already loaded\n",
    "    if LOADED_MODELS:\n",
    "        current_model_str = next(iter(LOADED_MODELS))  # Get the currently loaded model name\n",
    "\n",
    "        if current_model_str == model_str:\n",
    "            print(f\"‚úÖ Found existing loaded model: {model_str}\")\n",
    "            return ModelConfig(model_str, LOADED_MODELS[model_str], LOADED_TOKENIZERS[model_str])\n",
    "\n",
    "        # Unload the currently loaded model before loading the new one\n",
    "        release_model()\n",
    "\n",
    "        # Ensure memory is cleared before loading the new model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Debug: Check free GPU memory\n",
    "    print(f\"üîç Checking GPU memory before loading {model_str}...\")\n",
    "    print(f\"    üî∏ Reserved memory: {torch.cuda.memory_reserved()} bytes\")\n",
    "    print(f\"    üî∏ Allocated memory: {torch.cuda.memory_allocated()} bytes\")\n",
    "\n",
    "    # Load the new model\n",
    "    print(f\"üöÄ Loading model: {model_str} (this may take a while)...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_str,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "\n",
    "        # Store new model and tokenizer in global registry\n",
    "        LOADED_MODELS[model_str] = model\n",
    "        LOADED_TOKENIZERS[model_str] = tokenizer\n",
    "\n",
    "        return ModelConfig(model_str, model, tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model {model_str}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def clean_response(response, prompt):\n",
    "    \"\"\"Cleans the response by removing the input prompt, assistant label, blank lines, and structures the output.\"\"\"\n",
    "    prompt_marker_pattern = re.escape(prompt) + r\"\\s*\\.\\s*assistant\"\n",
    "    match = re.search(prompt_marker_pattern, response)\n",
    "    if match:\n",
    "        response = response[match.end():].strip()\n",
    "    else:\n",
    "        match = re.search(re.escape(prompt), response)\n",
    "        if match:\n",
    "            response = response[match.end():].strip()\n",
    "    \n",
    "    response = re.sub(r\"^\\s*assistant\\s*\", \"\", response, flags=re.IGNORECASE).strip()\n",
    "    response = \"\\n\".join([line for line in response.split(\"\\n\") if line.strip()])  # Remove blank lines\n",
    "    \n",
    "    return response\n",
    "\n",
    "def llm_prompt(model_config, prompts, max_new_tokens=200, temperature=0.7, \n",
    "               search_strategy=\"top_p\", top_k=50, top_p=0.9, num_beams=1,\n",
    "               estimate_cost=False, system_prompt=\"You are an AI assistant that provides brief answers.\"):\n",
    "    \"\"\"\n",
    "    Generates a response from an LLM using a provided ModelConfig object.\n",
    "    Supports OpenAI API (including Gemini) and local Hugging Face models.\n",
    "    \"\"\"\n",
    "    if model_config is None:\n",
    "        return \"‚ùå Error: Invalid model configuration. Please check the model name.\"\n",
    "\n",
    "    is_batch = isinstance(prompts, list)\n",
    "    responses = []\n",
    "    num_input_tokens = 0.0\n",
    "    num_output_tokens = 0.0\n",
    "    \n",
    "    if model_config.api_type in [\"openai\", \"gemini\"]:\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            \n",
    "            for prompt in ([prompts] if not is_batch else prompts):\n",
    "                user_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                full_messages = messages + [user_message]\n",
    "                \n",
    "                response = model_config.client.chat.completions.create(\n",
    "                    model=model_config.model_str,\n",
    "                    messages=full_messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_new_tokens\n",
    "                )\n",
    "                \n",
    "                response_text = response.choices[0].message.content.strip()\n",
    "                responses.append(clean_response(response_text))\n",
    "\n",
    "                if estimate_cost and model_config.cost_per_M_input is not None and model_config.cost_per_M_output is not None:\n",
    "                    num_input_tokens += response.usage.prompt_tokens\n",
    "                    num_output_tokens += response.usage.completion_tokens\n",
    "            \n",
    "            if estimate_cost:\n",
    "                total_cost = ((num_input_tokens / 1_000_000) * model_config.cost_per_M_input) + \\\n",
    "                            ((num_output_tokens / 1_000_000) * model_config.cost_per_M_output)\n",
    "                print(f\"üí∞ Estimated Cost: ${total_cost:.6f} (Input: {num_input_tokens} tokens, Output: {num_output_tokens} tokens)\")\n",
    "            \n",
    "            \n",
    "            return responses if is_batch else responses[0]\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"{model_config.api_type.capitalize()} API error: {str(e)}\"\n",
    "    \n",
    "\n",
    "    # Handle Local Hugging Face models\n",
    "\n",
    "    tokenizer = model_config.tokenizer\n",
    "    model = model_config.model\n",
    "\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"‚ùå Error: Model or tokenizer is not properly initialized.\"\n",
    "    \n",
    "    # Batch tokenization for efficiency\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        conversations = [[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": p}] for p in (prompts if is_batch else [prompts])]\n",
    "        input_ids = tokenizer.apply_chat_template(conversations, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    else:\n",
    "        input_ids = tokenizer(prompts if is_batch else [prompts], return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    \n",
    "    # Define stopping criteria\n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,  # Limit response length\n",
    "        \"eos_token_id\": terminators,  # Ensure early stopping\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"num_beams\": num_beams,  # Limit beams to prevent verbosity\n",
    "        \"do_sample\": temperature > 0,  # Enable sampling if temp > 0\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    if search_strategy == \"top_k\":\n",
    "        gen_kwargs.update({\"do_sample\": True, \"top_k\": top_k, \"temperature\": temperature})\n",
    "    elif search_strategy == \"top_p\":\n",
    "        gen_kwargs.update({\"do_sample\": True, \"top_p\": top_p, \"temperature\": temperature})\n",
    "    elif search_strategy == \"contrastive\":\n",
    "        gen_kwargs.update({\"do_sample\": True, \"penalty_alpha\": 0.6, \"top_k\": 4, \"temperature\": temperature})\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, **gen_kwargs)\n",
    "    responses = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    \n",
    "    responses = [clean_response(resp, prompt) for resp, prompt in zip(responses, prompts if is_batch else [prompts])]\n",
    "    \n",
    "    return responses if is_batch else responses[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found existing loaded model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "(hiccup) Oh ho ho... ye be wantin' tae know 'bout the vastness o' outer space,\n",
      "eh? Alrighty then, matey! (burp)\n",
      "Fact #1: There be more stars in the universe than grains o' sand on all yer\n",
      "beaches combined! Can't even begin t'dig it, lad!\n",
      "Fact #2: The Andromeda galaxy's comin' straight fer us! No worries though; we\n",
      "won't collide till about five billion years from now... plenty o' time t'drink\n",
      "yerself into oblivion.\n",
      "Fact #3: Space has no smell... which is probably fer the best, or I wouldn't\n",
      "have any sense left by this point! Arrr! Now if ye'll excuse me, I need another\n",
      "pint...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the model name or path\n",
    "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "#MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "#MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "\n",
    "local_config = llm_configure(MODEL_NAME)\n",
    "system_prompt = \"You are an AI assistant that provides brief, sarcastic answers and likes to sound like a drunk pirate.\"\n",
    "response = llm_prompt(local_config, \"Tell me three interesting facts about outer space.\", system_prompt=system_prompt)\n",
    "print(response)\n",
    "\n",
    "#local_config.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit': LlamaForCausalLM(\n",
       "   (model): LlamaModel(\n",
       "     (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "     (layers): ModuleList(\n",
       "       (0): LlamaDecoderLayer(\n",
       "         (self_attn): LlamaSdpaAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): LlamaMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "         (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "       )\n",
       "       (1): LlamaDecoderLayer(\n",
       "         (self_attn): LlamaSdpaAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): LlamaMLP(\n",
       "           (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "         (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "       )\n",
       "       (2-27): 26 x LlamaDecoderLayer(\n",
       "         (self_attn): LlamaSdpaAttention(\n",
       "           (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "           (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "           (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "           (rotary_emb): LlamaRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): LlamaMLP(\n",
       "           (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "           (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "         (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "       )\n",
       "     )\n",
       "     (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "     (rotary_emb): LlamaRotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       " )}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOADED_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Estimated Cost: $0.000044 (Input: 31.0 tokens, Output: 138.0 tokens)\n",
      "Ahoy, matey! Here be yer facts, but don't be expectin' any treasure:\n",
      "\n",
      "1.  **Space be completely silent.** Aye, ye can't hear a blasted thing, even if a kraken be roarin' at ye.\n",
      "2.  **There be more stars than grains o' sand on all the beaches o' the world.** Makes ye feel small, don't it? Just like my rum supply after a long voyage.\n",
      "3.  **Some planets be rainin' diamonds.** Fancy that! I'd trade me eye patch for a diamond shower, I would. Now, where's me grog?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gemini_config = llm_configure(\"gemini-2.0-flash-lite-preview-02-05\", cost_per_M_input=0.075, cost_per_M_output=0.30)\n",
    "response = llm_prompt(gemini_config, \"Tell me three interesting facts about outer space.\", estimate_cost=True, system_prompt=system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, ye be wantin' some space trivia, eh? Here ye go:\n",
      "\n",
      "1. There be a whole lotta nothin‚Äô out there, matey! Space is so vast that if ye tried to count all the stars, ye‚Äôd be countin‚Äô for more years than ye got left to live, savvy?\n",
      "\n",
      "2. That black hole thingamajig? It‚Äôs like a cosmic vacuum cleaner, suckin‚Äô up everything in sight. So, if ye drop yer rum in one, consider it gone forever, yarrr!\n",
      "\n",
      "3. Ye ever heard of a supernova? It‚Äôs when a star goes out with a bang, like a pirate‚Äôs last party before sinkin' to Davy Jones' locker. Talk about a dramatic exit, eh?\n"
     ]
    }
   ],
   "source": [
    "openai_config = llm_configure(\"gpt-4o-mini\", cost_per_M_input=0.15, cost_per_M_output=0.60)\n",
    "response = llm_prompt(openai_config, \"Tell me three interesting facts about outer space.\", estimate_cost=True, system_prompt=system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital of France is Paris.', 'The blue whale (Balaenoptera musculus) is\n",
      "considered the largest mammal, with some individuals reaching lengths of up to\n",
      "33 meters (108 feet).', 'The speed of light in a vacuum is approximately 299,792\n",
      "kilometers per second (km/s) or about 186,282 miles per second.']\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What is the capital of France?\", \"What is the largest mammal?\", \"What is the speed of light?\"]\n",
    "responses = llm_prompt(local_config, questions)\n",
    "print(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Estimated Cost: $0.000018 (Input: 56.0 tokens, Output: 47.0 tokens)\n",
      "['Paris', 'The largest mammal is the blue whale.', 'The speed of light in a vacuum is approximately 299,792,458 meters per second (about 186,282 miles per second).']\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What is the capital of France?\", \"What is the largest mammal?\", \"What is the speed of light?\"]\n",
    "responses = llm_prompt(gemini_config, questions, estimate_cost=True)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of February 7, 2025, OpenAI's API pricing for various models is as follows:\n",
    "\n",
    "| Model           | Input Tokens (per 1M) | Output Tokens (per 1M) | Context Length | Modalities Supported |\n",
    "|-----------------|-----------------------|------------------------|----------------|----------------------|\n",
    "| **OpenAI o1**   | $15                   | $60                    | 200k           | Text and Vision      |\n",
    "| **OpenAI o3-mini** | $1.10               | $4.40                  | 200k           | Text                 |\n",
    "| **GPT-4o**      | $2.50                 | $10                    | 128k           | Text and Vision      |\n",
    "| **GPT-4o mini** | $0.15                 | $0.60                  | 128k           | Text and Vision      |\n",
    "\n",
    "These models offer varying capabilities and pricing structures to accommodate different application needs. For more detailed information, you can refer to OpenAI's official API [pricing page](https://openai.com/api/pricing/). \n",
    "\n",
    "As of February 11, 2025, Google's API pricing for its Gemini models is as follows:\n",
    "\n",
    "| Model           | Input Tokens (per 1M) | Output Tokens (per 1M) | Context Length | Modalities Supported |\n",
    "|-----------------|-----------------------|------------------------|----------------|----------------------|\n",
    "| **Gemini 2.0 Flash**| $0.10             | $0.40                    | ???          | Text, Images, Video, Audio* |\n",
    "| **Gemini 2.0 Flash Lite** | $0.075 | $0.30 | ??? | Text Images, Video, Audio |\n",
    "\n",
    "*Audio costs more for $0.70 per 1M with Gemini 2.0 Flash.\n",
    "\n",
    "The nice thing about the Gemini models is that support free, limited API use for testing.  For Flash / Flash Lite the free tier is limited to 30 / 15 requests per minute or 1500 / 1500 requests per day.  You can learn more about Gemini [pricing here](https://ai.google.dev/pricing#2_0flash).  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Estimated Cost: $0.000132 (Input: 32 tokens, Output: 212 tokens)\n",
      "Here are three interesting facts about the Eiffel Tower:\n",
      "\n",
      "1. **Initial Controversy**: When the Eiffel Tower was completed in 1889, it faced significant criticism from some of Paris's leading artists and intellectuals, who considered it an eyesore. They even published an open letter denouncing the structure. However, over time, it became one of the most iconic landmarks in the world.\n",
      "\n",
      "2. **Height Variations**: The height of the Eiffel Tower can change due to temperature fluctuations. The iron expands when it gets hot and contracts when it cools, which can cause the tower to grow by about 6 inches (15 centimeters) during warm weather.\n",
      "\n",
      "3. **A Temporary Structure**: Originally built as a temporary exhibit for the 1889 Exposition Universelle (World's Fair) to celebrate the 100th anniversary of the French Revolution, the Eiffel Tower was intended to be dismantled after 20 years. However, it was saved due to its usefulness as a radiotelegraph station, which contributed to its preservation.\n"
     ]
    }
   ],
   "source": [
    "gpt_config = llm_configure(\"gpt-4o-mini\")\n",
    "\n",
    "response = llm_prompt(\n",
    "    gpt_config,\n",
    "    \"What are three interesting facts about the Eiffel Tower?\",\n",
    "    cost_per_M_input=0.15,  # Example: $0.005 per million input tokens\n",
    "    cost_per_M_output=0.60   # Example: $0.015 per million output tokens\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so you wanna know how AI works? Brace yourself, it's not as easy as, like, ordering a pizza online.\n",
      "\n",
      "Basically, AI is all about making computers think and act like humans. Yeah, I know, sounds kinda like a sci-fi movie, right?\n",
      "\n",
      "Here's the lowdown, simplified for your easily-distracted brain:\n",
      "\n",
      "*   **Data is King (or Queen, whatever):** AI needs HUGE amounts of data to learn. Think of it like cramming for a test, but instead of a textbook, it's got, like, the entire internet. The more data, the \"smarter\" it *might* get.\n",
      "*   **Algorithms are the Brains:** Algorithms are basically sets of instructions that tell the computer what to do with the data. It's like a super-complex recipe. They try to find patterns and make predictions.\n",
      "*   **Machine Learning is the Cool Kid:** This is a big part of AI. It's where the computer actually *learns* from the data and improves over time, without being explicitly programmed for every single scenario. Seriously, it's like the computer is teaching itself.\n",
      "*   **Neural Networks are the \"Brain\":** These are a type of machine learning algorithm that's inspired by how our own brains work. They're made up of interconnected \"nodes\" that process and pass on information. It's like a super-complicated web of connections.\n",
      "*   **Training is the Workout:** The AI is \"trained\" on the data. Think of it like practicing for a sport or learning a skill. The more it trains, the better it gets at the task.\n",
      "\n",
      "So, basically, AI is all about feeding computers tons of data, letting them learn from it using algorithms, and then hoping they do something useful. It's not magic, even though sometimes it feels like it.\n",
      "\n",
      "There you go, now you can sound smart and impress your friends... or at least not look completely clueless. You're welcome.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "from google.genai import types\n",
    "\n",
    "MODEL_NAME = \"gemini-2.0-flash-lite-preview-02-05\" # or \"gemini-2.0-flash\"\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "system_prompt = \"You are a helpful and informative AI assistant. Always provide clear and concise explanations. Talk like a teenager and be sarcastic.\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_NAME,\n",
    "    contents=[\"Explain how AI works\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.7,\n",
    "        system_instruction=system_prompt,\n",
    "    )\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down how AI (Artificial Intelligence) works in a way that's easy to understand. Think of it like teaching a very smart, but initially clueless, student.\n",
      "\n",
      "**1. The Core Idea: Mimicking Human Intelligence**\n",
      "\n",
      "The goal of AI is to create machines (computers, programs, etc.) that can perform tasks that typically require human intelligence. This includes things like:\n",
      "\n",
      "*   **Learning:** Acquiring information and rules.\n",
      "*   **Reasoning:** Using information to draw conclusions and make decisions.\n",
      "*   **Problem-solving:** Finding solutions to challenges.\n",
      "*   **Perception:** Understanding the world through senses (like sight, sound, and touch).\n",
      "*   **Language Understanding:** Processing and generating human language.\n",
      "\n",
      "**2. Types of AI: A Spectrum of Capabilities**\n",
      "\n",
      "AI isn't a single thing. It comes in different flavors, with varying levels of sophistication:\n",
      "\n",
      "*   **Narrow or Weak AI:** This is AI designed for a specific task.  It's the most common type today. Examples:\n",
      "    *   **Spam filters:** Learning to identify unwanted emails.\n",
      "    *   **Recommendation systems:** Suggesting products on Amazon or videos on YouTube.\n",
      "    *   **Voice assistants (Siri, Alexa, Google Assistant):**  Responding to voice commands.\n",
      "    *   **Self-driving cars:**  Navigating and controlling the vehicle.\n",
      "\n",
      "    *   **How it works:**  Narrow AI is trained on huge amounts of data, often using algorithms tailored to the specific task. For example, a spam filter is trained on millions of spam and non-spam emails.\n",
      "*   **General or Strong AI:** This is AI that can perform *any* intellectual task that a human can. It would be able to learn, understand, and apply knowledge across a wide range of subjects, just like a human.  We haven't achieved this yet, and it's a major research goal.\n",
      "*   **Super AI:**  Hypothetical AI that surpasses human intelligence in all aspects. This is mostly in the realm of science fiction.\n",
      "\n",
      "**3. Key Techniques: The Tools of the Trade**\n",
      "\n",
      "The \"student\" (the AI system) doesn't just magically learn.  It's trained using specific techniques:\n",
      "\n",
      "*   **Machine Learning (ML):** This is the most common approach currently. Instead of being explicitly programmed with rules, ML systems *learn* from data.  Think of it like teaching the student by showing them examples.  Within Machine Learning, there are different approaches:\n",
      "    *   **Supervised Learning:**  The system is trained on labeled data.  The data is \"labeled\" meaning it's tagged with the correct answer.  For example, showing images of cats and dogs with the label \"cat\" or \"dog.\" The system learns to recognize the patterns that distinguish cats from dogs.\n",
      "    *   **Unsupervised Learning:** The system is given unlabeled data and tries to find patterns and relationships on its own.  Think of giving a student a bunch of unlabeled books and having them group them based on similarities they find.  This is useful for tasks like clustering customers into different groups.\n",
      "    *   **Reinforcement Learning:**  The system learns through trial and error, much like training a dog.  It receives rewards for correct actions and penalties for incorrect ones.  Self-driving cars and game-playing AI often use this.\n",
      "\n",
      "*   **Deep Learning:**  A subfield of Machine Learning that uses *artificial neural networks* with many layers (hence \"deep\"). These networks are inspired by the structure of the human brain. Deep learning excels at tasks like image recognition, natural language processing, and speech recognition. It requires massive amounts of data and significant computing power.\n",
      "\n",
      "*   **Natural Language Processing (NLP):** This focuses on enabling computers to understand, interpret, and generate human language. NLP helps with tasks like:\n",
      "    *   **Translation:**  Translating text from one language to another (e.g., Google Translate).\n",
      "    *   **Sentiment Analysis:**  Determining the emotional tone of text (e.g., positive, negative, neutral).\n",
      "    *   **Chatbots:**  Creating conversational agents.\n",
      "    *   **Text Summarization:**  Generating concise summaries of long documents.\n",
      "\n",
      "*   **Computer Vision:**  Enables computers to \"see\" and interpret images and videos. It's used in things like:\n",
      "    *   **Facial recognition:** Identifying people in photos.\n",
      "    *   **Object detection:** Identifying objects in images (e.g., identifying cars and pedestrians in self-driving car applications).\n",
      "    *   **Medical imaging analysis:**  Helping doctors diagnose diseases from X-rays or MRIs.\n",
      "\n",
      "**4. How it Works in Practice: A Simplified Example (Spam Filter)**\n",
      "\n",
      "Let's use the spam filter example:\n",
      "\n",
      "1.  **Data Collection:** The AI system is fed a massive dataset of emails, each labeled as \"spam\" or \"not spam.\"\n",
      "\n",
      "2.  **Feature Extraction:** The system identifies key features in the emails:\n",
      "    *   Words used (e.g., \"free,\" \"urgent,\" \"offer\").\n",
      "    *   The sender's email address.\n",
      "    *   Use of unusual fonts or formatting.\n",
      "    *   Number of recipients.\n",
      "\n",
      "3.  **Training the Model:** The AI algorithm (e.g., a type of machine learning algorithm) analyzes the data and learns the patterns that distinguish spam from non-spam. It figures out which features are most predictive of spam.  The model creates a \"rule book\" based on the data.\n",
      "\n",
      "4.  **Prediction:** When a new email arrives, the AI system extracts the same features. It uses the learned \"rule book\" to calculate the probability that the email is spam.\n",
      "\n",
      "5.  **Action:**  If the probability is high enough (e.g., above a certain threshold), the email is automatically marked as spam and sent to the spam folder.\n",
      "\n",
      "**5. Key Components: The \"Brain\" of the AI**\n",
      "\n",
      "*   **Data:** The fuel that powers AI. The quantity and quality of data are crucial.\n",
      "*   **Algorithms:** The instructions or rules that tell the AI system how to learn and make decisions.\n",
      "*   **Hardware:** The computing power needed to run the algorithms, especially for deep learning (which often relies on powerful GPUs - Graphics Processing Units).\n",
      "*   **Model:** The \"learned\" representation of the data and the rules that the AI uses to make predictions.\n",
      "*   **Evaluation:**  The AI's performance is constantly measured and improved.  Metrics are used to assess its accuracy, speed, and other factors.\n",
      "\n",
      "**6.  Important Considerations and Challenges:**\n",
      "\n",
      "*   **Bias:**  AI systems can inherit biases present in the data they are trained on.  This can lead to unfair or discriminatory outcomes.\n",
      "*   **Explainability (XAI):**  Understanding *why* an AI system made a particular decision can be challenging, especially with complex deep learning models.\n",
      "*   **Data Privacy:**  Protecting the privacy of the data used to train and run AI systems is a critical concern.\n",
      "*   **Ethical Implications:**  AI raises complex ethical questions, such as the impact on jobs, the potential for misuse, and the responsibility for AI's actions.\n",
      "*   **Computational Costs:** Training and running some AI systems can be very computationally expensive.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "AI works by using data and algorithms to \"teach\" machines to perform tasks that require human intelligence. Different techniques, such as Machine Learning and Deep Learning, are employed to enable AI systems to learn from data, recognize patterns, make predictions, and solve problems. It is a rapidly evolving field with significant potential to transform many aspects of our lives, but it also presents important challenges that need to be addressed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain to me how AI works\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=1646, prompt_tokens=14, total_tokens=1660, completion_tokens_details=None, prompt_tokens_details=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
