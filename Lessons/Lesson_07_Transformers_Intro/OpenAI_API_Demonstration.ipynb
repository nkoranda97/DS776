{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from introdl.utils import wrap_print_text\n",
    "\n",
    "print = wrap_print_text(print) # overload print to wrap text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API Demonstration\n",
    "\n",
    "In this notebook we demonstrate how to access ChatGPT models programatically through OpenAI's python API.  If you want to experiment with the API you'll need to sign up for API account and pay for some credits.  It's really cheap so I encourage you to play around a bit.  The prices for recent models are given per 1 million tokens in the table below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of February 7, 2025, OpenAI's API pricing for various models is as follows:\n",
    "\n",
    "| Model           | Input Tokens (per 1M) | Output Tokens (per 1M) | Context Length | Modalities Supported |\n",
    "|-----------------|-----------------------|------------------------|----------------|----------------------|\n",
    "| **OpenAI o1**   | $15                   | $60                    | 200k           | Text and Vision      |\n",
    "| **OpenAI o3-mini** | $1.10               | $4.40                  | 200k           | Text                 |\n",
    "| **GPT-4o**      | $2.50                 | $10                    | 128k           | Text and Vision      |\n",
    "| **GPT-4o mini** | $0.15                 | $0.60                  | 128k           | Text and Vision      |\n",
    "\n",
    "These models offer varying capabilities and pricing structures to accommodate different application needs. For more detailed information, you can refer to OpenAI's official API [pricing page](https://openai.com/api/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be learning more about tokens in the coming weeks, but they're just numerical representations of text.\n",
    "\n",
    "The approximate ratio of words to tokens in English text varies depending on the complexity and style of the text. However, a commonly used estimate is:\n",
    "\n",
    "- **1 word ≈ 1.3 to 1.5 tokens** for general English text.\n",
    "\n",
    "This means that for every **1,000 words**, you can expect **1,300 to 1,500 tokens**. \n",
    "\n",
    "### Factors affecting the ratio:\n",
    "\n",
    "1. **Shorter words (e.g., \"a\", \"is\", \"the\")** tend to be single tokens.\n",
    "2. **Longer words (e.g., \"transformative\", \"neuroscientific\")** may be split into multiple tokens.\n",
    "3. **Punctuation and special characters** (e.g., `!`, `?`, `--`) often count as separate tokens.\n",
    "4. **Code, URLs, and non-standard text** typically have a higher token-to-word ratio.\n",
    "\n",
    "For OpenAI models like GPT, you can test this with `tiktoken`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 6, Token count: 7\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "text = \"This is a simple example sentence.\"\n",
    "tokens = encoding.encode(text)\n",
    "print(f\"Word count: {len(text.split())}, Token count: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the System Prompt and the User Prompt\n",
    "\n",
    "When interacting with the OpenAI API, two key types of prompts influence the model's responses: **the system prompt** and **the user prompt**.\n",
    "\n",
    "1. **System Prompt**  \n",
    "   - The system prompt is a message that sets the behavior and tone of the AI before the conversation begins.\n",
    "   - It provides instructions about how the AI should respond throughout the session.\n",
    "   - Example:  \n",
    "     ```json\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful and concise assistant.\"}\n",
    "     ```\n",
    "   - This helps guide the AI’s responses consistently across different user inputs.\n",
    "\n",
    "2. **User Prompt**  \n",
    "   - The user prompt is the actual message sent by the user to request information or perform a task.\n",
    "   - This is the main input that drives the AI’s response.\n",
    "   - Example:  \n",
    "     ```json\n",
    "     {\"role\": \"user\", \"content\": \"Explain black holes in simple terms.\"}\n",
    "     ```\n",
    "   - The AI will generate a response based on both the user’s request and the system prompt’s instructions.\n",
    "\n",
    "The system prompt **shapes** how the AI responds, while the user prompt **directs** the AI on what to answer.\n",
    "\n",
    "Unlike when we interact with a chatbot like ChatGPT the openAI API (and other large language models) are stateless.  They don't remember our previous interactions.  That means we need to send the system prompt and user prompt to the model each time.  If we're creating our own chatbot we'll also need to send conversation history.\n",
    "\n",
    "### Accessing the OpenAI API\n",
    "\n",
    "To access the OpenAI API you'll need to create an account and buy some credits.  Once your account is set up you'll need create an API key.  You generally don't want to share that key in a document such as this one so you can set it as an environment variable on your stystem.  If you want to play with the API in this class, you can add your API key to the file Lessons/Course_Tools/api_keys.env.  Copy it there without quotes and then run the cell below as we've done in other notebooks.  `config_paths_keys` checks your environment variables for `OPENAI_API_KEY` and if not present it sets the value from `api_keys.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n"
     ]
    }
   ],
   "source": [
    "from introdl.utils import config_paths_keys\n",
    "\n",
    "config_paths_keys();\n",
    "\n",
    "# or you could do this\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "# but you're encouraged to use env file for security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-mini Response:\n",
      "Here are three interesting facts about space:\n",
      "\n",
      "1. **The Universe is Expanding**: One of the most fascinating discoveries in\n",
      "modern astronomy is that the universe is expanding. Observations by astronomers,\n",
      "notably those of Edwin Hubble in the 1920s, showed that galaxies are moving away\n",
      "from us in all directions. This implies that the universe was once much smaller\n",
      "and denser, leading to the Big Bang theory, which explains the origin of the\n",
      "universe.\n",
      "\n",
      "2. **Time Dilation**: According to Einstein's theory of relativity, time is not\n",
      "absolute; it can be affected by gravity and speed. This phenomenon, known as\n",
      "time dilation, means that time moves slower in stronger gravitational fields or\n",
      "at higher speeds. For instance, astronauts on the International Space Station\n",
      "age slightly slower than people on Earth due to their high orbital velocity and\n",
      "the weaker gravitational field at that altitude.\n",
      "\n",
      "3. **It’s Mostly Empty**: Despite the vastness of space, most of it is\n",
      "incredibly empty. The average distance between stars in our galaxy is about 4.2\n",
      "light-years, which is about 25 trillion miles (40 trillion kilometers). In fact,\n",
      "if you could travel through the Milky Way at a speed of one light-year per year,\n",
      "it would take you over 100,000 years just to cross our galaxy, and that’s only\n",
      "one of billions of galaxies in the observable universe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a system prompt and a user prompt\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What are three interesting facts about space?\"\n",
    "\n",
    "# Call GPT-4o-mini with the latest API format\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    max_tokens=None # unlimited response length, set to integer to limit length\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(\"GPT-4o-mini Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
