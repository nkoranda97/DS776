{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Pooling Layers\n",
    "\n",
    "This notebook is intended to show you how parameters like kernel size, padding, and stride combine to determine the output size of convolutional and pooling layers.  When adapting network architectures to input images of sizes other than the original size you'll have to be able to determine the output sizes of the various layers in your network.\n",
    "\n",
    "## Convolutional Layers\n",
    "\n",
    "A key aspect of Conv2D layers is understanding how they modify the size and structure of the input image. The output size depends on several parameters such as kernel size, padding, and stride. Below we show the formula used to compute the output size, its motivation, and related concepts such as feature maps.\n",
    "\n",
    "### Formula for Output Size\n",
    "\n",
    "The formula for the output height and width of a Conv2D layer is:\n",
    "\n",
    "$$\\text{Output size} = \\left\\lfloor \\frac{\\text{Input size} + 2 \\times \\text{Padding} - \\text{Kernel size}}{\\text{Stride}} \\right\\rfloor + 1$$\n",
    "\n",
    "Where:\n",
    "- **Input size**: The height (or width) of the input image.\n",
    "- **Kernel size**: The height (or width) of the convolutional filter (kernel).\n",
    "- **Padding**: The number of pixels added around the input image.\n",
    "- **Stride**: The number of pixels by which the filter is moved across the input image.\n",
    "- **Floor**: The result is rounded down to the nearest integer. E.g. $\\lfloor 4.7 \\rfloor = 4$ and $\\lfloor -3.2 \\rfloor = -4$\n",
    "\n",
    "### Motivation for the Formula\n",
    "\n",
    "When a convolutional filter is applied to an image, it slides over the input image in both height and width. The size of the output depends on how the filter fits on the image and how much the filter moves (stride). \n",
    "\n",
    "- **Padding**: We may pad the input to control the spatial dimensions of the output, preserving the size or allowing the filter to \"see\" edge pixels more effectively.\n",
    "- **Stride**: Adjusting the stride controls how much the filter moves, which impacts the output size. A stride of 1 means the filter moves pixel by pixel, while a stride of 2 means it jumps every two pixels, resulting in a smaller output.  The larger the stride the smaller the output which is why the stride is in the denominator of our output size formula\n",
    "\n",
    "#### L02_2_Convolutional_Formulas Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l02_2_convolutional_formulas/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l02_2_convolutional_formulas/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/iDFKKdBxz74\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c1dbb9f2d2424eadfbab5c269b7bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntSlider(value=5, description='Input Size:', max=20, min=3), IntSlider(value=3,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96603e7d07b4b0e8f3e86ff27a6c1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# formula demo\n",
    "from conv_size_widget import create_widget\n",
    "create_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Maps and Channels\n",
    "\n",
    "Each convolution operation produces what is called a **feature map**. The feature map represents a set of features learned by applying the kernel to the input image.\n",
    "\n",
    "- **Number of Channels**: The input to a Conv2D layer can have multiple channels. For example, a color image has 3 channels (red, green, blue). A convolutional layer can apply multiple filters, each generating a separate feature map. If a layer has \\(N\\) filters, the result is \\(N\\) feature maps (output channels).\n",
    "\n",
    "- **Multiple Filters**: By using multiple filters, a convolutional layer can detect different types of patterns from the input, such as edges, corners, or textures. Each filter produces a distinct feature map, and these maps stack together to form the output with multiple channels.\n",
    "\n",
    "For example, if the input has 3 channels and we use 6 filters, the output will have 6 feature maps, each capturing different features across the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example 1: Single Channel Input with Single Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5, 5])\n",
      "Output shape: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example image (1 batch, 1 channel, 5x5 image)\n",
    "x = torch.rand(1, 1, 5, 5)\n",
    "\n",
    "# Conv2D layer: 3x3 kernel, stride 1, no padding\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "- Input shape: (1, 1, 5, 5) → 1 batch, 1 channel, 5x5 image\n",
    "- Conv2D layer: 1 input channel, 1 output channel (1 filter), 3x3 kernel, stride 1, padding 0\n",
    "- Output shape: (1, 1, 3, 3)\n",
    "\n",
    "Using the formula:\n",
    "\n",
    "$$\\text{Output size} = \\left\\lfloor \\frac{5 + 2(0) - 3}{1} \\right\\rfloor + 1 = 3$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example 2: RGB Image (3 Channels) with Multiple Filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 5, 5])\n",
      "Output shape: torch.Size([1, 6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example RGB image (1 batch, 3 channels, 5x5 image)\n",
    "x = torch.rand(1, 3, 5, 5)\n",
    "\n",
    "# Conv2D layer: 3x3 kernel, stride 1, no padding, 6 output channels (filters)\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- Input shape: (1, 3, 5, 5) → 1 batch, 3 channels (RGB), 5x5 image\n",
    "- Conv2D layer: 3 input channels, 6 output channels (filters), 3x3 kernel, stride 1, padding 0\n",
    "- Output shape: (1, 6, 3, 3) → 6 feature maps of size 3x3\n",
    "\n",
    "Each of the 6 output channels represents a feature map extracted by one of the filters.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example 3: Using Padding and Stride**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 8, 8])\n",
      "Output shape: torch.Size([1, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example image (1 batch, 1 channel, 8x8 image)\n",
    "x = torch.rand(1, 1, 8, 8)\n",
    "\n",
    "# Conv2D layer: 3x3 kernel, stride 2, padding 1, 4 output channels\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- Input shape: (1, 1, 8, 8) → 1 batch, 1 channel, 8x8 image\n",
    "- Conv2D layer: 1 input channel, 4 output channels (filters), 3x3 kernel, stride 2, padding 1\n",
    "- Output shape: (1, 4, 4, 4) → 4 feature maps of size 4x4\n",
    "\n",
    "Using the formula:\n",
    "\n",
    "$$\\text{Output size} = \\left\\lfloor \\frac{8 + 2(1) - 3}{2} \\right\\rfloor + 1 = 4$$\n",
    "\n",
    "Each of the 4 output channels represents a feature map extracted by one of the filters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "The formula and examples below apply to both maximum and average pooling.  The formula is similar to the one for Conv2D, but it doesn't involve the number of filters since pooling is applied to every input channel. \n",
    "\n",
    "### **Formula for Pooling Layer Output Size**\n",
    "\n",
    "For a 2D pooling operation (e.g., MaxPool2D or AvgPool2D), the output height and width are given by:\n",
    "\n",
    "\n",
    "$$\\text{Output size} = \\left\\lfloor \\frac{\\text{Input size} + 2 \\times \\text{Padding} - \\text{Kernel size}}{\\text{Stride}} \\right\\rfloor + 1$$\n",
    "\n",
    "Where:\n",
    "- **Input size**: The height (or width) of the input feature map.\n",
    "- **Kernel size**: The height (or width) of the pooling window.\n",
    "- **Padding**: The number of pixels added around the input.\n",
    "- **Stride**: The number of pixels the pooling window moves each step.\n",
    "- **Floor**: The result is rounded down to the nearest integer.\n",
    "\n",
    "### **Motivation for the Formula**\n",
    "\n",
    "Pooling layers are typically used for **downsampling**, which reduces the spatial dimensions of feature maps. Pooling works by sliding a window (of size `kernel_size`) over the input and taking the maximum or average value within the window, depending on the pooling type. The stride controls how far the window moves at each step.\n",
    "\n",
    "The formula for the output size of a pooling layer is the same basic idea as for convolution: it depends on how the window fits onto the input and how much it moves.\n",
    "\n",
    "### **Examples Using Pooling Layers in PyTorch**\n",
    "\n",
    "#### **Example 1: MaxPooling Without Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5, 5])\n",
      "Output shape: torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example image (1 batch, 1 channel, 5x5 image)\n",
    "x = torch.rand(1, 1, 5, 5)\n",
    "\n",
    "# MaxPool2D layer: 2x2 window, stride 2, no padding\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "# Apply the pooling\n",
    "output = pool(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Input size: 5x5\n",
    "- Kernel size: 2x2\n",
    "- Padding: 0\n",
    "- Stride: 2\n",
    "\n",
    "Using the formula:\n",
    "\n",
    "$$\\text{Output size} = \\left\\lfloor \\frac{5 + 2(0) - 2}{2} \\right\\rfloor + 1 = 2$$\n",
    "\n",
    "So the output will be a 2x2 feature map.\n",
    "\n",
    "#### **Example 2: MaxPooling with Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5, 5])\n",
      "Output shape: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example image (1 batch, 1 channel, 5x5 image)\n",
    "x = torch.rand(1, 1, 5, 5)\n",
    "\n",
    "# MaxPool2D layer: 3x3 window, stride 2, padding 1\n",
    "pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "# Apply the pooling\n",
    "output = pool(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Input size: 5x5\n",
    "- Kernel size: 3x3\n",
    "- Padding: 1\n",
    "- Stride: 2\n",
    "\n",
    "Using the formula:\n",
    "\n",
    "$$\\text{Output size} = \\left\\lfloor \\frac{5 + 2(1) - 3}{2} \\right\\rfloor + 1 = 3$$\n",
    "\n",
    "So the output will be a 3x3 feature map.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
