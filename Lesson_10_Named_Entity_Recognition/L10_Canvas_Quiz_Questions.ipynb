{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10-Question Multiple-Choice Reading Quiz for Chapter 4: \"Multilingual Named Entity Recognition (NER)\"**  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1. What is the main goal of Named Entity Recognition (NER)?**  \n",
    "A. To classify documents based on their sentiment.  \n",
    "B. To generate summaries of long text.  \n",
    "C. To identify and label entities such as names, locations, and organizations in text.  \n",
    "D. To predict the next word in a sequence.  \n",
    "**Answer**: C  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Which of the following is a significant challenge for multilingual NER?**  \n",
    "A. Limited availability of pre-trained models for English.  \n",
    "B. Excessive computational requirements for tokenizing short sequences.  \n",
    "C. Variability in script and handling out-of-vocabulary (OOV) words.  \n",
    "D. Lack of attention mechanisms for non-English languages.  \n",
    "**Answer**: C  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Which of the following multilingual transformer models is commonly used for NER?**  \n",
    "A. PEGASUS  \n",
    "B. GPT-2  \n",
    "C. BART  \n",
    "D. XLM-RoBERTa  \n",
    "**Answer**: D  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Why is SentencePiece tokenization advantageous for multilingual transformers?**  \n",
    "A. It unifies tokenization across scripts and languages.  \n",
    "B. It eliminates the need for pre-trained models.  \n",
    "C. It directly predicts entities without labels.  \n",
    "D. It requires less computational power than WordPiece.  \n",
    "**Answer**: A  \n",
    "\n",
    "---\n",
    "\n",
    "#### **5. What is the purpose of a classification head in a transformer model for NER?**  \n",
    "A. To predict a label for each token in the input text.  \n",
    "B. To aggregate features from multiple tokens into a single vector.  \n",
    "C. To train a separate model for multilingual support.  \n",
    "D. To apply convolutional layers for context aggregation.  \n",
    "**Answer**: A  \n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Which type of dataset is essential for training models for NER tasks?**  \n",
    "A. Balanced classes of sentiment labels.  \n",
    "B. Documents with extractive summaries.  \n",
    "C. Long sequences of text.  \n",
    "D. Entity-level annotated text.  \n",
    "**Answer**: D  \n",
    "\n",
    "---\n",
    "\n",
    "#### **7. How does zero-shot cross-lingual transfer benefit multilingual NER?**  \n",
    "A. It ensures models are trained on a single language.  \n",
    "B. It reduces the need for fine-tuning in new languages.  \n",
    "C. It limits the training process to high-resource languages.  \n",
    "D. It simplifies the architecture of multilingual transformers.  \n",
    "**Answer**: B  \n",
    "\n",
    "---\n",
    "\n",
    "#### **8. What is a unique challenge when using multilingual models for NER?**  \n",
    "A. Difficulty in tokenizing monolingual corpora.  \n",
    "B. Handling inconsistent annotations across languages.  \n",
    "C. Limited access to pre-trained SentencePiece tokenizers.  \n",
    "D. Inability to support long sequences.  \n",
    "**Answer**: B  \n",
    "\n",
    "---\n",
    "\n",
    "#### **9. What practical application can multilingual NER provide in real-world settings?**  \n",
    "A. Generating summaries for multilingual documents.  \n",
    "B. Translating text across multiple languages.  \n",
    "C. Extracting names and organizations from multilingual text for legal or business purposes.  \n",
    "D. Providing real-time chatbot responses.  \n",
    "**Answer**: C  \n",
    "\n",
    "---\n",
    "\n",
    "#### **10. Why are multilingual transformers effective for cross-lingual tasks?**  \n",
    "A. They tokenize text into fixed-length word embeddings.  \n",
    "B. They are pre-trained on diverse, multilingual corpora.  \n",
    "C. They limit their vocabulary size for improved efficiency.  \n",
    "D. They use convolutional layers to capture long-range dependencies.  \n",
    "**Answer**: B  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
